\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{tipa} % IPA
\usepackage{amsmath} % formulae
\usepackage{forest} % (non-dendrogram) trees
\usetikzlibrary{decorations.pathreplacing} % draw braces in tikz figure
\usepackage{standalone} % import figures
\usepackage{changepage} % adjust text width
\usepackage{parskip} % proper paragraphs, no indentation
\usepackage[margin=3cm, showframe]{geometry}
% TODO remove showframe

% \linespread{1.25}

\title{Clustering Dialect Varieties Based on Historical Sound Correspondences}
\author{Verena Blaschke}
\date{\today}

\begin{document}

\begin{titlepage}
\begin{center}

\vspace*{.1\textheight}

{\Large Bachelor's Thesis}
\vspace{2em}

\hrule
\vspace{0.6cm}
{\huge\bfseries
Clustering Dialect Varieties Based on Historical Sound Correspondences
}\\[0.7cm] 
\hrule
\vspace*{.05\textheight}
 
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} 
{\large
\textit{Author}\\
Verena Blaschke}\\
\href{mailto:verena.blaschke@student.uni-tuebingen.de}{\textit{verena.blaschke@student.uni-tuebingen.de}}\\
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright}
{\large
\textit{Supervisor}\\
Dr. Çağrı Çöltekin}\\
\href{mailto:ccoltekin@sfs.uni-tuebingen.de}{\textit{ccoltekin@sfs.uni-tuebingen.de}}\\
\end{flushright}
\end{minipage}\\

\vspace*{.1\textheight}

A thesis presented for the degree of\\
Bachelor of Arts\\
in\\
International Studies in Computational Linguistics

\vspace{2em}

Seminar für Sprachwissenschaft\\
Eberhard Karls Universität Tübingen

\vspace*{.1\textheight}
August 2018
\end{center}
  
\end{titlepage}




\pagenumbering{gobble}
% TODO abstract
\newpage
\tableofcontents
\listoftables
\listoffigures
\newpage
% TODO Eigenständigkeits- bzw. Antiplagiatserklärung  

\pagenumbering{arabic}


\section{Introduction}

% --- on clustering dialects and determining relevant features ---

clustering dialects and determining relevant features

\citet{prokic2012detecting}


\citet{heggarty2010splits} created a NeighborNet for Germanic languages, based on pronunciation differences between modern varieties. 
~conclude~ that this works because the pronunciation differences implicitly include information on sound changes that were not shared by all varieties

\citet{prokic2013combining}: judging regularity of correspondences via contingency table

% --- on historical sound correspondences ---

historical sound correspondences

% \citet{kondrak2009identication}

% --- on BSGC ---

Bipartite spectral graph co-clustering

introduced in \citet{dhillon2001co-clustering}

introduced as a method for dialect clustering in \citet{wieling2011bipartite}

\citet{wieling2013analyzing} applied this method for clustering British English dialects and compared the results to clusters obtained via PCA
 
\citet{montemagni2013synchronic} applied this method to Tuscan dialects and used left and right contexts for the sound segments

\citet{zha2001bipartite} introduced a similar (the same??) method as Dhillon at the same time, in another journal

% These algorithms and related ones were investigated by \citet{kluger2003spectral} for their use in bioinformatics as well (co-clustering genes and phenotypes).

\subsection{Continental West Germanic Doculects}

similarities not only because of genetic relatedness but also--enabled by the geographic proximity--mutual influences
cf. \citet[p. 8]{harbert2007germanic}

\citet[pp. 72-80]{nielsen1989germanic} gives an overview of the history of attempting to divide the West Germanic dialects into subgroups with the associated criteria (phonological, morphological, lexical, and/or extra-linguistic) and criticisms.

sound changes that characterize some but not all CWG variants:

- High German consonant shift
  - some notes. try \citet[pp. 47-48]{harbert2007germanic} or Beekes
  - \citet[p. 15]{harbert2007germanic}: mostly in Alpine German, partially in Franconian, not in Low German


\begin{figure}
\centering
\includestandalone[width=\textwidth]{figures/harbert}
\caption{Harbert 2007: somewhat taking into account the convergence of certain Germanic varieties due to mutual influence (tree + waves?)}
\label{fig:cwg_harbert}
\end{figure}


\begin{figure}
\begin{adjustwidth}{-3cm}{-3cm}
\centering
\scalebox{0.8}{
\input{figures/glottolog}
}
\end{adjustwidth}
% \includestandalone[width=\textwidth]{figures/glottolog}
\caption{Glottolog 3.2 for the doculects we used}
\end{figure}

\subsubsection{North Sea Germanic vs. Other}
\citet{stiles2013pan-west} posits that the most significant division of West Germanic varieties is the split into Ingv\ae{}onic (that is, North Sea Germanic) varieties and non-Ingv\ae{}onic varieties.

\subsubsection{German dialect groups by affricates}
\citet[pp. 182-199]{goblirsch2005lautverschiebungen}:

\begin{itemize}
  \item HG consonant shift: voiceless plosives to affricates 
  \begin{itemize}
    \item Suedoberdeutsch (Suedbairisch, Suedmittelbairisch, Suedalemannisch, parts of Mittelalemannisch): pf, ts, kx
    Ortisei
    \item Nordoberdeutsch (Nordbairisch, Mittelbairisch, Nordalemannisch, parts of Mittelalemannisch, Ostfraenkisch, Suedrheinfraenkisch).
    similar to Std German wrt the HGCS
    p, t - pf, ts; kV - aspirated
    \item Suedmitteldeutsch (Moselfraenkisch, Rheinfraenkisch, parts of Thueringisch, parts of Obersaechsisch, Schlesisch)
    \item Nordmitteldeutsch (Ripuarian, Nordhessisch, Nordthueringisch, Nordobersaechsisch):
    Vp, Vt, Vk - Vf, Vs, Vx; t - ts
    Cologne
  \end{itemize}
\end{itemize}

Biel, Hard, Graubuenden, Tuebingen, Walser - N/S Alemannic?

Herrlisheim?

Luxembourg?

not affected by HGCS: Dutch, Low German, Frisian

\subsubsection{Germanic varieties by obstruent systems}
\citet[pp. 215-235]{goblirsch2005lautverschiebungen}: "Typology of modern Germanic obstruent systems", from least to most innovative

\begin{itemize}
  \item aspirationless:
  East Swedisch; Dutch, S Low German, Nordenglisch, Scots(, Frisian); Normitteldeutsch
  no consistent sound shift
  \item general:
  Swedish, Norwegian; English, Low German, NE Dutch
  \item voiceless:
  E Danish, S Swedish, SW Norwegian; North Frisian; Suedmitteldeutsche Dialekte ohne Lenisierung
  \item length:
  Oberdeutsche Dialekte ohne Lenisierung
  \item aspiration:
  Danish; N Low German; High German dialects with lenition
\end{itemize}


\newpage
\section{Data}

continental European West Germanic dialects (CWG) and standard languages

\citet{heggarty2018sound} compiled IPA transcriptions of word lists in a range of Germanic doculects for his Sound Comparisons project.
From this database, we used 110 cognate sets from 20 modern CWG doculects and a reconstruction of Proto-Germanic.
For the phonetic alignment step (see section~\ref{s:alignments}), we used 14 additional doculects that are Germanic but not continental West Germanic. 
To control for transciber bias, we only worked with doculects that shared the same transcriptor, Warren Maguire.
We excluded one CWG doculect that covered only 35 concepts. % Schaeddel (Frisian)
The Proto-Germanic data cover all 110 concepts; each of the modern doculects covers at least 103 concepts, and each concept is covered by at least 17 modern doculects.
In total, we have 2181 sequence alignments between Proto-Germanic and modern CWG doculects.

note on inflected forms

\newpage
\section{Methods}

\subsection{Alignments}
\label{s:alignments}

In order to extract sound correspondences, we need to align the sound sequences corresponding to each concept first.
We carry out alignment based on data from all the investigated doculects at once using multiple sequence alignment.
Using multiple sequence alignment instead of carrying out pairwise alignments between the modern data and the reference data makes it possible to carry out the alignment based on patterns found in commonalities between the modern doculects as well.
Because of this, we use all of the Germanic data we extracted from the Sound Comparisons project instead of only the CWG doculects and Proto-Germanic.

We use a library-based version \citep{notredame2000t-coffee:} of the progressive multiple sequence alignment method \citep{thompson1994clustal}.
For each concept:

\begin{enumerate}
\item
Divide the phonetic representation of each word into an array of sound segments.
These sound segments are typically single IPA tokens (including diacritics), but we use multi-token segments for affricates, diph- and triphthongs, and geminates.

\item
Create alignments for all possible binary sequence combinations.
These alignments are created using the algorithm from \citet{needleman1970general},
with a scoring scheme based on the sound classes introduced by \citet{list2012sca}.
All segment alignments from this step are stored in a library,
each associated with a weight reflecting its relative frequency.
% TODO. in how far does it reflect probable sound class changes in the LingPy implementation (vs. relative frequency)

\item
Use the binary calculations to create a distance matrix between the sequences.
Convert the distance matrix into a tree using the UPGMA method \citep{sokal1958statistical}.

\item 
Progressing from the tips of the tree to the root,
consecutively join the alignments meeting at branchings,
until (at the root) all alignments have been consolidated into one alignment table.
Alignments are merged using the library created in the first step.
\end{enumerate}

We use the LingPy library for Python \citep{list2018lingpy} to perform these steps.
Table~\ref{tab:msa} shows an excerpt from the multiple sequence alignment for the concept ``cold''.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|llllll}
\hline
Doculect       & \multicolumn{6}{l}{Sound segments} \\ \hline
Proto-Germanic  & k    & a    & l   & d    & a  & z  \\
Ortisei        & k\textsuperscript{h}   & \textopeno    & l   & \texttoptiebar{ts}  & -  & - \\ 
Biel           & \textchi    & \textscripta\textupsilon   & -   & t    & -  & -  \\
Walser         & x    & a\textlengthmark    & l   & t    & -  & -  \\
Luxembourg     & k\textsuperscript{h}   & a\textlengthmark   & l   & -    & -  & -  \\
Westerkwartier & k\textsuperscript{h}   & o    & \textltilde   & t\textsuperscript{h}   & -  & -  \\ \hline
\end{tabular}
\end{center}
\label{tab:msa}
\caption{An excerpt from the aligned sequence table for the concept ``cold''.}
\end{table}

\subsection{Sound Correspondences}

After performing sound segment-wise alignment,
we extract sound correspondences between each
modern doculect and Proto-Germanic from the alignment table.
We use straightforward segment-to-segment correspondences
as well as correspondences that include contextual information:

\begin{itemize}
\item
\textbf{No context}:
These are simple segment-to-segment correspondences.
For the example given in Table~\ref{tab:msa},
the correspondences for the Ortisei doculect are:
k $>$ k\textsuperscript{h}, a $>$ \textopeno, l $>$ l, d $>$ \texttoptiebar{ts}, a $>$ $\emptyset$, and z $>$ $\emptyset$.

\item
\textbf{Simple context}:
We (separately) add information about the
left and right single-segment context,
stating whether the context is a
word boundary (\#), gap ($\emptyset$), consonant (C), or vowel (V). 
This can only be performed when the context in question is of
the same type for both Proto-Germanic and the modern doculect.

\item
\textbf{Sound class-based context}:
This is similar to the previous category,
but we give more fine-grained information about consonants and vowels.
We use the sound classes introduced by \citet{list2012sca},
which discern between fifteen consonant groups and sixteen vowel groups.

\end{itemize}

% TODO check montemagni. also ref them here

We ignore gap-gap alignments, as they are 
ignore swaps (swapped segments are aligned with gaps) -- happen only in 3 of the 111 terms

dealing with gaps, don't include word boundaries multiple times

For each doculect, we ignore segment correspondences that occur less than three times across all concepts to reduce the effect misalignments can have.

\begin{table}[h]
\begin{center}
\begin{tabular}{l|llllll}
\hline
       & \multicolumn{6}{l}{Sound segments and inferred correspondences} \\ \hline
Pr.-Germ.  & k    & a    & l   & d    & a  & z  \\
Ortisei        & k\textsuperscript{h}   & \textopeno    & l   & \texttoptiebar{ts}  & -  & - \rule[-2mm]{0pt}{0pt}\\\hline
No context & k $>$ k\textsuperscript{h} & a $>$ \textopeno & l $>$ l & d $>$ \texttoptiebar{ts} & a $>$ $\emptyset$ & z $>$ $\emptyset$ \rule{0pt}{4mm}\\[3mm]
Simple & k $>$ k\textsuperscript{h} / \#\_ & a $>$ \textopeno / C\_ & l $>$ l / V\_ & d $>$ \texttoptiebar{ts} / C\_ & a $>$ $\emptyset$ / C\_ & \\
context & k $>$ k\textsuperscript{h} / \_V & a $>$ \textopeno{} / \_C & l $>$ l / \_C & & & z $>$ $\emptyset$ / \_\# \\[3mm]
Sound class- & k $>$ k\textsuperscript{h} / \#\_ & a $>$ \textopeno / K\_ &  l $>$ l / A\_ & d $>$ \texttoptiebar{ts} / L\_ & & \\
based context & & a $>$ \textopeno{} / \_L & & & & z $>$ $\emptyset$ / \_\# \\
\hline
\end{tabular}
\end{center}
\label{tab:corres}
\caption{An excerpt from the aligned sequence table for the concept ``cold''.}
\end{table}

\newpage
\subsection{Clustering}

We implemented two approaches to custering the data.
Both clustering approaches consist of three steps: ..normalizing.. the data, dimensionality reduction, and hierarchical clustering.
Each approach is carried out once with only the context-less sound correspondences, and once with all types of correspondences.

\subsubsection{TF-IDF/SVD and k-means}

% TF-IDF conceptually similar to first step of BSGC: adjusting feature frequencies by how informative they are

We first TF-IDF transform the doculect-by-correspondence tally matrix.

% TODO sources for tfidf?

TF (term frequency) is calculated as follows

\begin{equation*}
\operatorname{tf}(doculect_i, corres_j) =
\frac{\text{number of occurrences of } corres_j \text{ in } doculect_i}
{\text{number of sound correspondences in } doculect_i}
\end{equation*}

and IDF (inverse document frequency) is calculated as

\begin{equation*}
\operatorname{idf}(corres_j) =
\frac{\text{number of doculects}}
{\text{number of doculects with } corres_j}
\end{equation*}
.

To combine term frequency and inverse document frequency and transform the tally matrix, 
we use the implementation by \cite{pedregosa2011scikit-learn}, 
which is defined as

\begin{equation*}
\operatorname{tf-idf}(doculect_i, corres_j) =
\text{tf}(doculect_i, corres_j)
\times
(
log
(\text{idf}(corres_j))
+ 1)
\end{equation*}

We then apply truncated singular vector decomposition (preserve a large enough number of components to explain at least 85\% of the variance)
remove noise



\subsubsection{Bipartite spectral graph co-clustering}

The method, as taken from \citet{dhillon2001co-clustering}:

\begin{enumerate}
\item 
Given a (binary) co-occurrence matrix $A^{m \times n}$ ($m$ = number of doculects, $n$ = number of sound correspondences), normalize this matrix.
First, create two diagonal matrices $D_1^{m \times m}$ and $D_2^{n x n}$ that, respectively, contain the row sums/column sums of $A$.
Compute the diagonal matrices' inverses. 
% Since they are diagonal, this is very easy, but only possible if no entries on their diagonals are 0.
Then, compute the square roots of the inverses
% (also easy because of the diagonal structure)
.
Finally, create the normalized matrix $A_n = D_1^{-\frac{1}{2}} \times A \times D_2^{-\frac{1}{2}}$.
Effectively: divide each entry by the square root of the sum of its row's entries and by the square root of the sum of its column's entries.
This reduces the importance of doculects/correspondences that co-occur with a large number of other entries.

\item
Perform SVD on $A_n$ to get the left and right singular vectors $u_i$ and $v_i$.
We ignore the singular vectors belonging to the first/largest singular value, and take the second singular vectors ($u_2$, $v_2$).
(If clustering with $k > 2$, also skip the first singular vectors and take the $log(k)/log2$ following vectors.)
\citet{kluger2003spectral}: The first singular vectors only "make a trivial constant contribution to the matrix".
However, trying this out for the toy example in `wieling2011bipartite`, it is very much the case that when using the first singular vectors, the resulting vector (after step 3) contains the same value for all entries, being maximally unhelpful for k-means clustering (step 4).

\item
Calculate $D_1^{-\frac{1}{2}} \times u_2$ and $D_2^{-\frac{1}{2}} \times v_2$, and concatenate them to get the vector/matrix $Z$. 

\item
Perform k-means clustering on Z.

% \item
% If performing hierarchical clustering, repeat steps 1-4 on all clusters individually.
\end{enumerate}


\subsection{Ranking sound correspondences by importance}

Representativeness: How many doculects in this cluster exhibit this sound correspondence? \citep{wieling2011bipartite}
Distinctiveness: How often does a sound correspondence occur in this cluster compared to other clusters? \citep{wieling2011bipartite}
Frequency: Is it a rare or frequent correspondence? could maybe be combined with regularity, especially since the chance for mis-alignments is a lot higher for rare correspondences

Ranking the sound correspondences for each cluster
(BSGC: correspondences that are in the cluster,
TF-IDF: correspondences that are exhibited by the cluster's doculects)

introduced in \citet{wieling2011bipartite}

For each cluster, rank the associated sound correspondences by the following metrics:

\textbf{Representativeness}.
How many doculects in this cluster exhibit this sound correspondence?

\begin{equation*}
\operatorname{representativeness}(cluster_i, corres_j) = 
\frac{\text{number of doculects in } cluster_i \text{ with }  corres_j}
{\text{number of doculects in }  cluster_i}
\end{equation*}

\textbf{Distinctiveness}.
How often does a sound correspondence occur in this cluster compared to other clusters? 
This requires two additional measures: 

\begin{equation*}
\operatorname{relative\_occurrence}(cluster_i, corres_j) = 
\frac{\text{number of doculects in } cluster_i \text{ with }  corres_j}
{\text{total number of doculects with } corres_j}
\end{equation*}

and

\begin{equation*}
\operatorname{relative\_size}(cluster_i) = 
\frac{\text{number of doculects in } cluster_i}
{\text{total number of doculects}}
\end{equation*}

Then, 

\begin{equation*}
\operatorname{distinctiveness}(cluster_i, corres_j) = 
\frac{\operatorname{relative\_occurrence}(cluster_i, corres_j) - \operatorname{relative\_size}(cluster_i)}
{1 - \operatorname{relative\_size}(cluster_i)}
\end{equation*}

\textbf{Importance}
\citet{wieling2011bipartite}: 
The average of representativeness and distinctiveness.

We use the harmonic mean instead (penalize cases were one value is very high but the other is very low)

\begin{equation*}
\operatorname{importance}(cluster_i, corres_j) = 
\frac{
2 * \operatorname{rep}(cluster_i, corres_j) * \operatorname{dist}(cluster_i, corres_j)}
{\operatorname{rep}(cluster_i, corres_j) + \operatorname{dist}(cluster_i, corres_j)}
\end{equation*}

We additionally re-rank correspondences with the same importance score by the number of occurrences in a cluster; more frequent correspondences rank higher.

% TODO maybe: try out harmonic mean instead. I don't like how correspondences with low representativeness/distinctiveness but 100% of the other metric still get pretty high scores. Using the harmonic mean would give them a score closer to the lower score, I think.

According to \citet{wieling2010hierarchical}, the values of the second right singular vector are a good substitute for the above metrics.

\newpage
\section{Results}

\begin{figure}
    \begin{adjustwidth}{-0.6cm}{-0.5cm}
    \centering
    \includestandalone[height=0.28\textheight]{figures/tfidf-nocontext}
    \hspace{-5mm}
    \includestandalone[height=0.28\textheight]{figures/tfidf-context}
    \end{adjustwidth}
    \caption{TFIDF with (right) and without (left) context information, highest-rating correspondence per non-singleton cluster (with $\geq$80\% importance score). truncated svd: right}
    \label{fig:tfidf-nocontext}
\end{figure}

\begin{figure}
  \includestandalone[width=\textwidth]{figures/bsgc-nocontext}
  \caption{BSGC without context information and highest-rating correspondence per non-singleton cluster (with $\geq$80\% importance score)}
  \label{fig:bsgc-nocontext}
\end{figure}

\begin{figure}
  \includestandalone[width=\textwidth]{figures/bsgc-context}
  \caption{BSGC context and highest-rating correspondence per non-singleton cluster (with $\geq$80\% importance score)}
  \label{fig:bsgc-context}
\end{figure}


highest-ranked sound correspondences
(+ example words?)
% TODO update!

bsgc-context: $\geq$90\% imp for non-singleton clusters:

bsgc-nocontext: highest (non-singleton) score: 88.00  (rep: 78.57, dist: 100.00), one singleton with a corres of 100\%
$\geq$80\% imp for non-singleton clusters:


svd how many components. svd vs no svd


\newpage
\section{Discussion}

% -- specific ---

\subsection{Comparison with Glottolog tree}
comparison with language ancestry data from \citet{hammarstrom2018glottolog}

comparison with sound shifts (e.g. High German consonant shift)

\subsection{(grouping dialects by) sound shifts}

BSGC mostly picks up on s $>$ \textesh

\subsection{effect of context information}

really helps for BSGC: higher importance values! (more values $\geq$80, scores $\geq$90 at all)

\subsection{co-clustering}
How beneficial is co-clustering really? Why not just cluster the doculects (after TF-IDF maybe) and then apply the sound correspondence metrics? It seems like the benefit of co-clustering is mainly that $v_2$ can be used for ranking the correspondences, although this doesn't seem to be very popular among the authors who used bipartite spectral graph clustering for dialect data...

why does bsgc not work that well here despite it working quite nicely for wieling, montemagni, ...?
- number of samples
- doculects/language family ("tighter")
- narrowness of transcription

wrongly assigned sound correspondences (k means initialization?)

comparison to the results discussed in \citet{wieling2011bipartite}
They remark on a common alignment [-]:[\textesh], which commonly appears after [t]:[t]. Interpreting affricates as single segments with the result of correspondences such as [t]:[\t{t\textesh}], or using another approach to include contextual information seems more satisfying to me.

% --- more general ---

All of the comparisons here are phonetic (and might possibly include some morphological information in some cases) and on a word level, but we are ignoring lexical, syntactical, morphological, etc. information in this analysis.

To what extend does clustering dialects and/or applying a hierarchical model to dialect data make sense? (tree vs. web, `vertical' changes vs. `horizontal' influences)
data is from a relatively large region though

more abstract sound correspondence rules (incl. context) to model sound changes more generally (nasalization, lenition)

Regularity: How often does this correspondence occur compared to the original sound? similar to \citet{prokic2013combining}. also like bigram MLE

investigate further the influence of data selection/preprocessing/properties on bsgc performance

\bibliographystyle{chicago}
\bibliography{lib}
\end{document}

% NOTES
% intros to SVD, k-means, tf-idf, alignments?

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{tipa}
\usepackage{forest}
\usetikzlibrary{decorations.pathreplacing}
\usepackage[margin=1.3in]{geometry}

\title{Clustering Dialect Varieties Based on Historical Sound Correspondences}
\author{Verena Blaschke}
\date{\today}

\begin{document}
\pagenumbering{gobble}
\maketitle
% TODO abstract
\newpage
% TODO Eigenständigkeits- bzw. Antiplagiatserklärung  

\pagenumbering{arabic}


\section{Introduction}

% --- on clustering dialects and determining relevant features ---

clustering dialects and determining relevant features

\cite{prokic2012detecting}


\cite{heggarty2010splits} created a NeighborNet for Germanic languages, based on pronunciation differences between modern varieties. 
~conclude~ that this works because the pronunciation differences implicitly include information on sound changes that were not shared by all varieties

\cite{prokic2013combining}: judging regularity of correspondences via contingency table

% --- on historical sound correspondences ---

historical sound correspondences

\cite{kondrak2009identication}

% --- on BSGC ---

Bipartite spectral graph co-clustering

introduced in \cite{dhillon2001co-clustering}

introduced as a method for dialect clustering in \cite{wieling2011bipartite}

\cite{wieling2013analyzing} applied this method for clustering British English dialects and compared the results to clusters obtained via PCA
 
\cite{montemagni2013synchronic} applied this method to Tuscan dialects and used left and right contexts for the sound segments

\cite{zha2001bipartite} introduced a similar (the same??) method as Dhillon at the same time, in another journal

% This algorithms and related ones were investigated by \cite{kluger2003spectral} for their use in bioinformatics as well (co-clustering genes and phenotypes).
 
\section{Data}

continental European West Germanic dialects (CWG) and standard languages

\subsection{actual data}

\cite{heggarty2018sound} compiled IPA transcriptions of word lists in a range of Germanic doculects.
From this database, we used 110 cognate sets from 21 modern continental West Germanic doculects and a reconstruction of Proto-Germanic.
For the phonetic alignment step (see section~\ref{s:alignments}), we used 14 additional doculects that are Germanic but not continental West Germanic. 
To control for transciber bias, we only worked with doculects that shared the same transcriptor, Warren Maguire.

segmentation: affricates, diph-/triphthongs

some doculects do not have (cognate) entries for some entries. give some statistics about the coverage?

\subsection{notes on continental West Germanic dialects/languages}

similarities not only because of genetic relatedness but also--enabled by the geographic proximity--mutual influences
cf. \cite[p. 8]{harbert2007germanic}


sound changes that characterize some but not all CWG variants:

- High German consonant shift
  - some notes. try \cite[pp. 47-48]{harbert2007germanic} or Beekes
  - \cite[p. 15]{harbert2007germanic}: mostly in Alpine German, partially in Franconian, not in Low German

- North-Sea-Germanic sound shifts/differences NSG vs other West Germanic dialects: \cite{stiles2013pan-west}

\begin{figure}
\centering
\begin{forest}
for tree={
  parent anchor=south, 
  child anchor=north,
%   l sep=1cm,
%   s sep=1cm
}
[West Germanic
  [North Sea Germanic
    [Anglo-Frisian
        [Frisian]
    ] 
    [Old Saxon
        [Low German]
    ]
  ]
  [Franconian
    [Low Franconian
        [Dutch]
    ]
    [High Franconian, name=a]
  ]
  [Alpine Germanic
    [Alemannic]
    [Bavarian, name=b]
  ]
]
\draw[decorate,decoration={brace,mirror}]
  (a.south west) -- node[below] {Middle High German} (b.south east);
\end{forest}
\caption{Harbert 2007: somewhat taking into account the convergence of certain Germanic varieties due to mutual influence (tree + waves?)}
\label{fig:cwg_harbert}
\end{figure}

\begin{figure}
\centering
\scalebox{0.8}{
\begin{forest}
for tree={
  parent anchor=south, 
  child anchor=north,
%   l sep=1cm,
%   s sep=1cm
}
[West Germanic
  [North Sea Germanic
    [Anglo-Frisian
        [Frisian]
    ] 
    [Old Saxon
        [Low German]
    ]
  ]
  [Franconian
    [Low Franconian
        [Dutch]
    ]
    [High Franconian
        [German
            [Standard German]
            [Alsatian]
        ]
        [Middle Franconian
            [Luxembourgish]
            [Ripurarian]
        ]
    ]
  ]
  [High German
    [Alpine Germanic
        [Alemannic
            [Swiss German]
            [Swabian]
            [Walser]
        ]
        [Bavarian]
    ]
  ]
]
\end{forest}
}
\caption{Glottolog 3.2: entirely a tree}
\label{fig:cwg_glottolog}
\end{figure}


\section{Methods}

\subsection{Alignments}
\label{s:alignments}

% TODO. dataset
% 4 P-G words in BDPA
Since the Proto-Germanic words are not aligned with the BDPA entries, we need to align them first. 
We can carry out alignment based on data from all the investigated doculects at once using multi-sequence alignment.
% TODO details (all steps) & double-check
For this, we use a library-based version \cite{notredame2000t-coffee:} of the progressive multi-sequence alignment method \cite{thompson1994clustal}.
For each concept:

% "global" mode: \cite{needleman1970general}
% "SCA" model: \cite{list2012sca:}

% TODO either double-check (& possibly extend these details), or remove them (& just name them) since they are not the focus of this work
\begin{enumerate}
\item
Create alignments for all possible binary combinations of sequences.
These alignments are created using the Needleman-Wunsch algorithm (\cite{needleman1970general}) and based on the sound class method by \cite{list2012sca:}.
All segment alignments from this step are stored in a library, each together with a corresponding sound-class based weight. % TODO check the nature of these weights

\item
Use the binary calculations to create a distance matrix between the sequences.
Convert the distance matrix into a tree.

\item 
Progressing from the tips of the tree to the root, join the closest two alignments at each step.
Merge alignments using the library created in the first step.
\end{enumerate}

This method is performed using the implementation in the LingPy library for Python (\cite{list2018lingpy}).
For the multi-sequence alignment, we use all of the Germanic BDPA data instead of just the CWG doculects, because this yielded better results in preliminary experiments. % TODO vague?
Table ~\ref{tab:msa}

\begin{table}[]
\begin{center}
\begin{tabular}{l|llllll}
Doculect       & \multicolumn{6}{l}{Sound segments} \\ \hline
Proto-Germanic & x   & a     & n   & d   & u   & z  \\
Biel           & h   & ɒ     & ŋ   & -   & -   & -  \\
Std. German    & h   & a     & n   & t   & -   & -  \\
Vesterkolonien & h   & æ̃ː    & -   & t   & -   & - 
\end{tabular}
\end{center}
\label{tab:msa}
\caption Extract of the aligned sequence table for the concept ``hand''.
\end{table}

We then extract the pairwise sound correspondences between Proto-Germanic and each doculect from the alignments.

ignore gap-gap alignments caused by the inclusion of additional doculects for the MSA step
ignore swaps (swapped segments are aligned with gaps) -- happen only in 3 of the 111 terms

% TODO motivation
In addition to the simple correspondences, we also extract sound correspondences with some contextual information.
To do this, we pad each sequence with `#' to represent word boundaries.
% TODO more

% TODO include left and right context separately?
% a : æ̃ː, (C, a, C) : (C, æ̃ː, C), (G, a, N) : (H, æ̃ː, -)

\subsection{Clustering}

\subsubsection{Bipartite spectral graph co-clustering}

The method, as taken from \cite{dhillon2001co-clustering}:

\begin{enumerate}
\item 
Given a (binary) co-occurrence matrix $A^{m \times n}$ ($m$ = number of doculects, $n$ = number of sound correspondences), normalize this matrix.
First, create two diagonal matrices $D_1^{m \times m}$ and $D_2^{n x n}$ that, respectively, contain the row sums/column sums of $A$.
Compute the diagonal matrices' inverses. 
% Since they are diagonal, this is very easy, but only possible if no entries on their diagonals are 0.
Then, compute the square roots of the inverses
% (also easy because of the diagonal structure)
.
Finally, create the normalized matrix $A_n = D_1^{-\frac{1}{2}} \times A \times D_2^{-\frac{1}{2}}$.
Effectively: divide each entry by the square root of the sum of its row's entries and by the square root of the sum of its column's entries.
This reduces the importance of doculects/correspondences that co-occur with a large number of other entries.

\item
Perform SVD on $A_n$ to get the left and right singular vectors $u_i$ and $v_i$.
We ignore the singular vectors belonging to the first/largest singular value, and take the second singular vectors ($u_2$, $v_2$).
(If clustering with $k > 2$, also skip the first singular vectors and take the $log(k)/log2$ following vectors.)
\cite{kluger2003spectral}: The first singular vectors only "make a trivial constant contribution to the matrix".
However, trying this out for the toy example in `wieling2011bipartite`, it is very much the case that when using the first singular vectors, the resulting vector (after step 3) contains the same value for all entries, being maximally unhelpful for k-means clustering (step 4).

\item
Calculate $D_1^{-\frac{1}{2}} \times u_2$ and $D_2^{-\frac{1}{2}} \times v_2$, and concatenate them to get the vector/matrix $Z$. 

\item
Perform k-means clustering on Z.

% \item
% If performing hierarchical clustering, repeat steps 1-4 on all clusters individually.
\end{enumerate}


\subsubsection{TF-IDF/SVD and k-means}

TF-IDF conceptually similar to first step of BSGC: adjusting feature frequencies by how informative they are

\begin{equation*}
\operatorname{tf-idf}(t, d) =
\frac{\text{number of occurrences of } t \text{ in } d}
{\text{number of tokens in } d}
\times
log
\frac{\text{number of documents}}
{\text{number of documents with } t}
\end{equation*}
% TODO src

\subsection{Ranking sound correspondences by importance}

Ranking the sound correspondences for each cluster
(BSGC: correspondences that are in the cluster,
TF-IDF: correspondences that are exhibited by the cluster's doculects)

introduced in \cite{wieling2011bipartite}

For each cluster, rank the associated sound correspondences by the following metrics:

\textbf{Representativeness}.
How many doculects in this cluster exhibit this sound correspondence?

\begin{equation*}
\operatorname{representativeness}(cluster_i, corres_j) = 
\frac{\text{number of doculects in } cluster_i \text{ with }  corres_j}
{\text{number of doculects in }  cluster_i}
\end{equation*}

\textbf{Distinctiveness}.
How often occurs a sound correspondence in this cluster compared to other clusters? 
This requires two additional measures: 

\begin{equation*}
\operatorname{relative\_occurrence}(cluster_i, corres_j) = 
\frac{\text{number of doculects in } cluster_i \text{ with }  corres_j}
{\text{total number of doculects with } corres_j}
\end{equation*}

and

\begin{equation*}
\operatorname{relative\_size}(cluster_i) = 
\frac{\text{number of doculects in } cluster_i}
{\text{total number of doculects}}
\end{equation*}

Then, 

\begin{equation*}
\operatorname{distinctiveness}(cluster_i, corres_j) = 
\frac{\operatorname{relative\_occurrence}(cluster_i, corres_j) - \operatorname{relative\_size}(cluster_i)}
{1 - \operatorname{relative\_size}(cluster_i)}
\end{equation*}

\textbf{Importance}
The average of representativeness and distinctiveness.

\begin{equation*}
\operatorname{importance}(cluster_i, corres_j) = 
\frac{\operatorname{representativeness}(cluster_i, corres_j) + \operatorname{distinctiveness}(cluster_i, corres_j)}
{2}
\end{equation*}

% TODO maybe: try out harmonic mean instead. I don't like how correspondences with low representativeness/distinctiveness but 100% of the other metric still get pretty high scores. Using the harmonic mean would give them a score closer to the lower score, I think.

According to \cite{wieling2010hierarchical}, the values of the second right singular vector are a good substitute for the above metrics.

\section{Results}

\subsection{Comparison with Glottolog tree}
comparison with language ancestry data from \cite{hammarstrom2018glottolog}

\subsection{Sound correspondences}
comparison of highest-ranked sound correspondences
(+ example words?)

comparison with sound shifts (e.g. High German consonant shift)

effect of context information

\section{Discussion}

% -- specific ---

is the context beneficial?

How beneficial is co-clustering really? Why not just cluster the doculects (after TF-IDF maybe) and then apply the sound correspondence metrics? It seems like the benefit of co-clustering is mainly that $v_2$ can be used for ranking the correspondences, although this doesn't seem to be very popular among the authors who used bipartite spectral graph clustering for dialect data...

comparison to the results discussed in \cite{wieling2011bipartite}
They remark on a common alignment [-]:[\textesh], which commonly appears after [t]:[t]. Interpreting affricates as single segments with the result of correspondences such as [t]:[\t{t\textesh}], or using another approach to include contextual information seems more satisfying to me.

% --- more general ---

All of the comparisons here are phonetic (and might possibly include some morphological information in some cases) and on a word level, but we are ignoring lexical, syntactical, morphological, etc. information in this analysis.

To what extend does clustering dialects and/or applying a hierarchical model to dialect data make sense? (tree vs. web, `vertical' changes vs. `horizontal' influences)
data is from a relatively large region though

\bibliographystyle{chicago}
\bibliography{lib}
\end{document}

% NOTES
% intros to SVD, k-means, tf-idf, alignments?
